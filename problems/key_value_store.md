## Problem Statement
**Key-value stores** are **distributed hash tables (DHTs)**. A key is generated by the hash function and should be unique. 
In a key-value store, a key binds to a specific value and doesn’t assume anything about the structure of the value. 
A value can be a blob, image, server name, or anything the user wants to store against a unique key.
## Requirements
- **Functional Requirements**
  - The size of a key-value pair is small: less than 10 KB.
  - Ability to store big data
  - Tunable consistency
  - Ability to always write: The applications should always have the ability to write into the key-value storage.
  - Hardware heterogeneity: The system shouldn’t have distinguished nodes.  Each node should be functionally able to 
    do any task.
- **Non-Functional Requirements**
  - High availability: The system responds quickly, even during failures.
  - High scalability: The system can be scaled to support large data set.
  - Fault tolerance: The key-value store should operate uninterrupted despite failures in servers or their components.
## API Design
* The get function
```
get(key)
```
It’s the key against which we want to get value.
* The put function
```
put(key, value)
```
It stores the value associated with the key
## Database Design

## Basic System Design and Algorithm
### Data Partitioning for high scalability

* Two challenges while partitioning the data:
  * Distribute data across multiple servers evenly.
  * Minimize data movement when nodes are added or removed.
* **Simple Distributed Hashing technique** is not efficient to distribute load across multiple store nodes is not scalable 
  nodes as we want to add and remove nodes with minimal change in our infrastructure. But in simple hashing method, 
  when we add or remove a node, we need to move a lot of keys
* We will use Consistent Hashing to partition data which has the following advantages:
  * **Automatic scaling**: servers could be added and removed automatically depending on the load.
  * **Heterogeneity**: the number of virtual nodes for a server is proportional to the server capacity.  For example, 
    servers with higher capacity are assigned with more virtual nodes.

### Data Replication
* Various methods to replicate the storage:
  * Primary-secondary approach
    * In a primary-secondary approach, one of the storage areas is primary, and other storage areas are secondary.
    * The secondary replicates its data from the primary. The primary serves the write requests while the secondary 
      serves read requests.
    * Primary in single point of failure,
  * Peer-to-peer approach
    * In the peer-to-peer approach, all involved storage areas are primary, and they replicate the data to stay updated. 
    * Both read and write are allowed on all nodes
    * Usually, it’s inefficient and costly to replicate in all n nodes.
* We will use a peer-to-peer relationship for replication.
* We will define a replication factor ``n`` to replicate it to ``n nodes``. N is generally 3 or 5.
* A coordinator node is assigned the key ``K``. It will handle all reads and write operations.
  * Replicate the keys to ``n-1`` successors on the ring (clockwise)
  * We will skip those virtual nodes which has same physical nodes. Choose unique server in different data centres.

### Consistency
* Since data is replicated at multiple nodes, it must be synchronized across replicas. 
* Quorum consensus can guarantee consistency for both read and write operations
  * If ``W + R > N``, strong consistency is guaranteed (Usually N = 3, W = R = 2). 
  * If ``W + R <= N``, strong consistency is not guaranteed.
* For key-value stores, we prefer availability over the consistency.
* Dynamo and Cassandra adopt eventual consistency, which is our recommended consistency model for our key-value store.

### Data Versioning
### Handle Failures
## References
* https://www.educative.io/courses/grokking-modern-system-design-interview-for-engineers-managers/system-design-the-key-value-store

